#!/usr/bin/env python
"""
åˆ†æfiltered_predictions.jsonä¸­çš„è¡¥ä¸ï¼Œä½¿ç”¨Claude APIæˆ–OpenAI APIè¿›è¡Œè¯„ä¼°ï¼Œ
å¹¶å°†åˆ†æç»“æœç›´æ¥æ·»åŠ åˆ°åŸå§‹filtered_predictions.jsonæ–‡ä»¶ä¸­
"""

import os
import json
import sys
import time
import glob
import argparse
import concurrent.futures
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = Path(__file__).parent
# ç¡®ä¿èƒ½å¯¼å…¥claudeæ¨¡å—
sys.path.insert(0, str(current_dir))

# å¯¼å…¥Claude APIå®¢æˆ·ç«¯
from claude import ClaudeAPI, extract_content

# OpenAI APIé…ç½®(ç¡¬ç¼–ç )
OPENAI_BASE_URL = "your_api_base"
OPENAI_API_KEY = "api_key""
OPENAI_MODEL = "gpt-4o"

# å¯¼å…¥OpenAI API(å¦‚æœå¯ç”¨)
try:
    import openai
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ä¸ºçº¿ç¨‹å®‰å…¨æ“ä½œæ·»åŠ é”
save_lock = threading.Lock()

def load_filtered_predictions(file_path: str) -> Dict[str, Any]:
    """
    åŠ è½½è¿‡æ»¤åçš„é¢„æµ‹æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŠ è½½çš„JSONæ•°æ®
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        sys.exit(1)

def save_predictions(predictions: Dict[str, Any], file_path: str) -> None:
    """
    ä¿å­˜é¢„æµ‹æ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        predictions: é¢„æµ‹æ•°æ®
        file_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        with save_lock:  # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç„¶åé‡å‘½åçš„æ–¹å¼ï¼Œé¿å…å†™å…¥ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå
            temp_file = file_path + ".tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(predictions, f, ensure_ascii=False, indent=2)
            os.replace(temp_file, file_path)
            print(f"é¢„æµ‹æ•°æ®å·²ä¿å­˜åˆ° {file_path}")
    except Exception as e:
        print(f"ä¿å­˜æ•°æ®åˆ° {file_path} æ—¶å‡ºé”™: {e}")

def get_prompt_template() -> str:
    """
    è·å–åˆ†æè¡¥ä¸çš„æç¤ºæ¨¡æ¿
    
    Returns:
        æç¤ºæ¨¡æ¿å­—ç¬¦ä¸²
    """
    return """
You are an AI assistant specialized in analyzing code patches. I will provide a GitHub issue (problem_statement) and a corresponding patch. Your task is to analyze this patch and provide detailed insights that could help develop an alternative solution.

Follow these steps:
1. Analyze the patch file and understand the changes made
2. Determine the core methods and techniques used to solve the problem
3. Identify the main files and sections that were modified
4. Identify key assumptions and limitations in the current solution

Return your analysis in JSON format with the following fields:
- approach_summary: Summary of the main approach used in the first solution
- modified_files: List of files that were modified
- key_changes: Description of key code changes in the patch
- strategy: The core solution strategy at an abstract level
- specific_technique_from_first_solution: Specific technique used that should be avoided in alternative solutions
- specific_files_or_functions: Files or functions that should not be modified in the same way
- assumptions_made_in_first_solution: Assumptions made in the first solution
- component_not_touched_in_first_solution: Components or key functions not touched but potentially relevant
- different_perspective: A different perspective for looking at the problem

The following examples are provided only for reference to illustrate the expected level of detail and abstraction for each field. Your analysis should be based on your own understanding of the patch and problem:

approach_summary example: "Added a conditional check to handle MultiOutputClassifier by accessing classes through the estimators_ attribute"
modified_files example: ["sklearn/model_selection/_validation.py"]
key_changes example: "Added a condition to check if estimator has 'estimators_' attribute, then uses estimator.estimators_[i_label].classes_ instead of estimator.classes_[i_label] for MultiOutputClassifier"
strategy example: "Component-specific exception handling" (instead of "Interface extension to provide unified attribute access")
specific_technique_from_first_solution example: "Direct attribute checking with hasattr() and conditional branching"
specific_files_or_functions example: "_fit_and_predict function in sklearn/model_selection/_validation.py"
assumptions_made_in_first_solution example: "Assumes that only MultiOutputClassifier needs special handling for classes_ attribute access"
component_not_touched_in_first_solution example: "MultiOutputClassifier class in sklearn/multioutput.py which could implement classes_ attribute directly"
different_perspective example: "API consistency perspective: make MultiOutputClassifier conform to the same interface as other classifiers instead of modifying the validation module"

Problem:
{problem_statement}
Patch:
{model_patch}
"""

def analyze_patch_with_openai(
    problem_statement: str, 
    model_patch: str, 
    instance_id: str, 
    api_key: str = OPENAI_API_KEY,
    base_url: str = OPENAI_BASE_URL,
    model: str = OPENAI_MODEL
) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI APIåˆ†æè¡¥ä¸
    
    Args:
        problem_statement: é—®é¢˜æè¿°
        model_patch: æ¨¡å‹è¡¥ä¸
        instance_id: å®ä¾‹ID
        api_key: OpenAI APIå¯†é’¥
        base_url: OpenAI APIçš„åŸºç¡€URL
        model: æ¨¡å‹åç§°
        
    Returns:
        OpenAIçš„åˆ†æç»“æœ
    """
    if not OPENAI_AVAILABLE:
        print("é”™è¯¯: æœªå®‰è£…openaiåº“ï¼Œè¯·ä½¿ç”¨ pip install openai è¿›è¡Œå®‰è£…")
        return {"error": "æœªå®‰è£…openaiåº“"}
    
    # è·å–æç¤ºæ¨¡æ¿å¹¶å¡«å……
    prompt_template = get_prompt_template()
    prompt = prompt_template.format(
        problem_statement=problem_statement,
        model_patch=model_patch
    )
    
    print(f"ğŸ“ æ­£åœ¨ä½¿ç”¨OpenAI APIåˆ†æ {instance_id}...")
    
    retry_count = 0
    max_retries = 3
    
    while retry_count <= max_retries:
        try:
            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
            client = OpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=120  # è®¾ç½®2åˆ†é’Ÿè¶…æ—¶
            )
            
            # è°ƒç”¨OpenAI API
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000
            )
            
            # æå–å†…å®¹
            content = response.choices[0].message.content
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # å¦‚æœè¿”å›çš„å†…å®¹åŒ…å«å›´ç»•JSONçš„é¢å¤–æ–‡æœ¬ï¼Œæå–JSONéƒ¨åˆ†
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                
                if start_idx >= 0 and end_idx > start_idx:
                    json_content = content[start_idx:end_idx]
                    return json.loads(json_content)
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨OpenAIå“åº”ä¸­æ‰¾åˆ°JSONæ ¼å¼å†…å®¹: {content[:100]}...")
                    return {"error": "æ— æ³•è§£æJSON", "raw_content": content}
            except json.JSONDecodeError as e:
                print(f"âš ï¸ è§£æOpenAI JSONå“åº”æ—¶å‡ºé”™: {e}")
                return {"error": "JSONè§£æé”™è¯¯", "raw_content": content}
                
        except Exception as e:
            retry_count += 1
            if retry_count <= max_retries:
                wait_time = 5 * retry_count  # æŒ‡æ•°é€€é¿
                print(f"âš ï¸ è°ƒç”¨OpenAI APIæ—¶å‡ºé”™ (å°è¯• {retry_count}/{max_retries}): {e}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"âŒ è°ƒç”¨OpenAI APIå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                return {"error": f"OpenAI APIé”™è¯¯: {str(e)}"}

def analyze_patch_with_claude(problem_statement: str, model_patch: str, instance_id: str, claude_api: ClaudeAPI) -> Dict[str, Any]:
    """
    ä½¿ç”¨Claude APIåˆ†æè¡¥ä¸
    
    Args:
        problem_statement: é—®é¢˜æè¿°
        model_patch: æ¨¡å‹è¡¥ä¸
        instance_id: å®ä¾‹ID
        claude_api: Claude APIå®¢æˆ·ç«¯
        
    Returns:
        Claudeçš„åˆ†æç»“æœ
    """
    # è·å–æç¤ºæ¨¡æ¿å¹¶å¡«å……
    prompt_template = get_prompt_template()
    prompt = prompt_template.format(
        problem_statement=problem_statement,
        model_patch=model_patch
    )
    
    print(f"ğŸ“ æ­£åœ¨ä½¿ç”¨Claude APIåˆ†æ {instance_id}...")
    
    retry_count = 0
    max_retries = 3
    
    while retry_count <= max_retries:
        try:
            response = claude_api.send_message(
                message=prompt,
                model="claude-3-7-sonnet-20250219",
                temperature=0.3,
                max_tokens=4000
            )
            
            content = extract_content(response)
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # å¦‚æœClaudeè¿”å›çš„å†…å®¹åŒ…å«å›´ç»•JSONçš„é¢å¤–æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦æå–JSONéƒ¨åˆ†
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ–¹æ³•ï¼šå¯»æ‰¾ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}ä¹‹é—´çš„å†…å®¹
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                
                if start_idx >= 0 and end_idx > start_idx:
                    json_content = content[start_idx:end_idx]
                    return json.loads(json_content)
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°JSONæ ¼å¼å†…å®¹: {content[:100]}...")
                    return {"error": "æ— æ³•è§£æJSON", "raw_content": content}
            except json.JSONDecodeError as e:
                print(f"âš ï¸ è§£æJSONå“åº”æ—¶å‡ºé”™: {e}")
                return {"error": "JSONè§£æé”™è¯¯", "raw_content": content}
                
        except Exception as e:
            retry_count += 1
            if retry_count <= max_retries:
                wait_time = 5 * retry_count  # æŒ‡æ•°é€€é¿
                print(f"âš ï¸ è°ƒç”¨Claude APIæ—¶å‡ºé”™ (å°è¯• {retry_count}/{max_retries}): {e}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"âŒ è°ƒç”¨Claude APIå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                return {"error": f"Claude APIé”™è¯¯: {str(e)}"}

def process_single_entry(
    entry_data: Tuple[str, Dict[str, Any]], 
    predictions: Dict[str, Any],
    conclusion_file: str,
    api_type: str,
    claude_api: Optional[ClaudeAPI] = None,
    force_reprocess: bool = False
) -> None:
    """
    å¤„ç†å•ä¸ªæ¡ç›®
    
    Args:
        entry_data: åŒ…å«é”®å’Œæ•°æ®çš„å…ƒç»„
        predictions: é¢„æµ‹æ•°æ®å­—å…¸
        conclusion_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        api_type: APIç±»å‹ï¼Œ'claude'æˆ–'openai'
        claude_api: Claude APIå®¢æˆ·ç«¯(å½“api_typeä¸º'claude'æ—¶ä½¿ç”¨)
        force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²æœ‰åˆ†æçš„æ¡ç›®
    """
    key, instance_data = entry_data
    
    # å¦‚æœå·²ç»å¤„ç†è¿‡è¿™ä¸ªé”®ä¸”ä¸å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Œè·³è¿‡
    if "claude_analysis" in instance_data and not force_reprocess:
        print(f"è·³è¿‡å·²å¤„ç†çš„æ¡ç›®: {key}")
        return
    elif "claude_analysis" in instance_data and force_reprocess:
        print(f"å¼ºåˆ¶é‡æ–°å¤„ç†æ¡ç›®: {key}")
    
    # æå–é—®é¢˜é™ˆè¿°å’Œè¡¥ä¸
    if "problem_statement" not in instance_data:
        print(f"è­¦å‘Š: {key} æ²¡æœ‰é—®é¢˜é™ˆè¿°ï¼Œè·³è¿‡")
        return
        
    problem_statement = instance_data["problem_statement"]
    model_patch = instance_data["model_patch"]
    
    try:
        # æ ¹æ®APIç±»å‹è°ƒç”¨ç›¸åº”çš„åˆ†æå‡½æ•°
        if api_type == 'claude' and claude_api:
            analysis = analyze_patch_with_claude(problem_statement, model_patch, key, claude_api)
        elif api_type == 'openai':
            analysis = analyze_patch_with_openai(problem_statement, model_patch, key)
        else:
            print(f"é”™è¯¯: æ— æ•ˆçš„APIé…ç½®")
            return
        
        # æ·»åŠ åˆ†æç»“æœå¹¶ä¿å­˜
        with save_lock:
            predictions[key]["claude_analysis"] = analysis
            # ä¿å­˜æ—¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç„¶åé‡å‘½åï¼Œé¿å…æ–‡ä»¶æŸåé£é™©
            temp_file = conclusion_file + ".tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(predictions, f, ensure_ascii=False, indent=2)
            os.replace(temp_file, conclusion_file)
            print(f"âœ… æ¡ç›® {key} å¤„ç†å®Œæˆå¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ å¤„ç†æ¡ç›® {key} æ—¶å‡ºé”™: {str(e)}")
        return None

def process_conclusion_file(
    conclusion_file: str, 
    api_type: str, 
    claude_api: Optional[ClaudeAPI] = None, 
    test_mode: bool = False,
    max_workers: int = 3,
    force_reprocess: bool = False
) -> None:
    """
    å¤„ç†å•ä¸ªconclusion.jsonæ–‡ä»¶ï¼Œä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç†
    
    Args:
        conclusion_file: conclusion.jsonæ–‡ä»¶è·¯å¾„
        api_type: APIç±»å‹ï¼Œ'claude'æˆ–'openai'
        claude_api: Claude APIå®¢æˆ·ç«¯(å½“api_typeä¸º'claude'æ—¶ä½¿ç”¨)
        test_mode: æ˜¯å¦ä»…æµ‹è¯•ç¬¬ä¸€æ¡æ•°æ®
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²æœ‰åˆ†æçš„æ¡ç›®
    """
    print(f"å¤„ç†æ–‡ä»¶: {conclusion_file}")
    
    # åŠ è½½conclusion.jsonæ•°æ®
    predictions = load_filtered_predictions(conclusion_file)
    
    # ç¡®å®šè¦å¤„ç†çš„é”®åˆ—è¡¨
    keys_to_process = list(predictions.keys())
    if test_mode:
        # ä»…å¤„ç†ç¬¬ä¸€ä¸ªæ¡ç›®
        keys_to_process = keys_to_process[0:1]
    
    # å‡†å¤‡éœ€è¦å¤„ç†çš„æ¡ç›®åˆ—è¡¨
    entries_to_process = []
    for key in keys_to_process:
        # å¦‚æœå¼ºåˆ¶é‡æ–°å¤„ç†æˆ–æ²¡æœ‰åˆ†æè¿‡ï¼Œåˆ™æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
        if force_reprocess or "claude_analysis" not in predictions[key]:
            entries_to_process.append((key, predictions[key]))
    
    if not entries_to_process:
        print(f"æ–‡ä»¶ {conclusion_file} ä¸­æ‰€æœ‰æ¡ç›®å·²å¤„ç†ï¼Œè·³è¿‡")
        return
    
    print(f"æ–‡ä»¶ {conclusion_file} ä¸­æœ‰ {len(entries_to_process)} æ¡å¾…å¤„ç†")
    
    # ä½¿ç”¨é¡ºåº + å¹¶å‘æ–¹å¼å¤„ç†æ¡ç›®ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    batch_size = min(max_workers, len(entries_to_process))
    processed_count = 0
    
    for i in range(0, len(entries_to_process), batch_size):
        batch = entries_to_process[i:i+batch_size]
        print(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(entries_to_process) + batch_size - 1)//batch_size}ï¼Œå…± {len(batch)} æ¡")
        
        # ä½¿ç”¨ThreadPoolExecutorå¤„ç†å½“å‰æ‰¹æ¬¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = []
            for entry_data in batch:
                futures.append(
                    executor.submit(
                        process_single_entry,
                        entry_data,
                        predictions,
                        conclusion_file,
                        api_type,
                        claude_api,
                        force_reprocess
                    )
                )
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result(timeout=180)  # è®¾ç½®3åˆ†é’Ÿè¶…æ—¶
                    processed_count += 1
                except concurrent.futures.TimeoutError:
                    print(f"âš ï¸ å¤„ç†æ¡ç›®è¶…æ—¶")
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ¡ç›®æ—¶å‡ºé”™: {e}")
        
        # æ‰¹æ¬¡é—´ç­‰å¾…ï¼Œé¿å…APIé™æµ
        if i + batch_size < len(entries_to_process):
            wait_time = 2  # æ¯æ‰¹æ¬¡é—´ç­‰å¾…2ç§’
            print(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç­‰å¾… {wait_time} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
            time.sleep(wait_time)
    
    print(f"æ–‡ä»¶ {conclusion_file} å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count}/{len(entries_to_process)} æ¡æ•°æ®")

def find_conclusion_files(base_path: str) -> list:
    """
    æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„conclusion.jsonæ–‡ä»¶
    
    Args:
        base_path: åŸºç¡€è·¯å¾„
        
    Returns:
        æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    conclusion_files = []
    
    # éå†5ä¸ªdefaultæ–‡ä»¶å¤¹
    for i in range(1, 6):
        folder_pattern = f"{base_path}/default_{i}/*/"
        timestamp_folders = glob.glob(folder_pattern)
        
        for timestamp_folder in timestamp_folders:
            conclusion_file = os.path.join(timestamp_folder, "conclusion.json")
            if os.path.exists(conclusion_file):
                conclusion_files.append(conclusion_file)
    
    return conclusion_files

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="ä½¿ç”¨LLM APIåˆ†æè¡¥ä¸å¹¶å°†ç»“æœæ·»åŠ åˆ°conclusion.jsonæ–‡ä»¶")
    parser.add_argument("--base_path", default="/home/uaih3k9x/swebench/evolve_agent/newest_exp_claude37_30-125",
                        help="åŸºç¡€è·¯å¾„ï¼ŒåŒ…å«5ä¸ªdefaultæ–‡ä»¶å¤¹")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†æ¯ä¸ªæ–‡ä»¶çš„ç¬¬ä¸€æ¡æ•°æ®")
    parser.add_argument("--api_type", choices=["claude", "openai"], default="claude",
                        help="ä½¿ç”¨çš„APIç±»å‹ï¼šclaudeæˆ–openai(é»˜è®¤)")
    parser.add_argument("--claude_api_key", default="api_key",
                        help="Claude APIå¯†é’¥")
    parser.add_argument("--max_workers", type=int, default=3, 
                        help="æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º3")
    parser.add_argument("--force", action="store_true",
                        help="å¼ºåˆ¶é‡æ–°å¤„ç†å·²æœ‰åˆ†æçš„æ¡ç›®")
    args = parser.parse_args()
    
    # æ ¹æ®APIç±»å‹å¤„ç†
    if args.api_type == 'claude':
        # åˆå§‹åŒ–Claude APIå®¢æˆ·ç«¯
        claude_api = ClaudeAPI(args.claude_api_key)
    else:  # openai
        if not OPENAI_AVAILABLE:
            print("é”™è¯¯: æœªå®‰è£…openaiåº“ï¼Œè¯·ä½¿ç”¨ pip install openai è¿›è¡Œå®‰è£…")
            return
        claude_api = None
    
    # æŸ¥æ‰¾æ‰€æœ‰conclusion.jsonæ–‡ä»¶
    conclusion_files = find_conclusion_files(args.base_path)
    print(f"æ‰¾åˆ° {len(conclusion_files)} ä¸ªconclusion.jsonæ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼ˆæ–‡ä»¶ä¹‹é—´æ˜¯é¡ºåºå¤„ç†çš„ï¼‰
    for file_path in conclusion_files:
        process_conclusion_file(
            file_path, 
            args.api_type, 
            claude_api, 
            args.test,
            args.max_workers,
            args.force
        )
        
    print("æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main() 