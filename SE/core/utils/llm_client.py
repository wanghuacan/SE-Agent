#!/usr/bin/env python3
"""
LLMå®¢æˆ·ç«¯æ¨¡å—
ä¸ºSEæ¡†æ¶æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£
"""

from openai import OpenAI
from typing import Dict, Any, Optional, List
from core.utils.se_logger import get_se_logger


class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹"""
    
    def __init__(self, model_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«name, api_base, api_keyç­‰
        """
        self.config = model_config
        self.logger = get_se_logger("llm_client", emoji="ğŸ¤–")
        
        # éªŒè¯å¿…éœ€çš„é…ç½®å‚æ•°
        required_keys = ["name", "api_base", "api_key"]
        missing_keys = [key for key in required_keys if key not in model_config]
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®å‚æ•°: {missing_keys}")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼
        self.client = OpenAI(
            api_key=self.config["api_key"],
            base_url=self.config["api_base"],
        )
        
        self.logger.info(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯: {self.config['name']}")
    
    def call_llm(self, messages: List[Dict[str, str]], 
                 temperature: float = 0.3, 
                 max_tokens: Optional[int] = None) -> str:
        """
        è°ƒç”¨LLMå¹¶è¿”å›å“åº”å†…å®¹
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
            
        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹
            
        Raises:
            Exception: LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„max_output_tokensä½œä¸ºé»˜è®¤å€¼
            if max_tokens is None:
                max_tokens = self.config.get("max_output_tokens", 4000)
            
            self.logger.debug(f"è°ƒç”¨LLM: {len(messages)} æ¡æ¶ˆæ¯, temp={temperature}, max_tokens={max_tokens}")
            
            # ä½¿ç”¨åŸºæœ¬çš„OpenAIå®¢æˆ·ç«¯è°ƒç”¨ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼
            # ä¸ä½¿ç”¨é¢å¤–å‚æ•°ï¼Œé¿å…æœåŠ¡å™¨é”™è¯¯
            response = self.client.chat.completions.create(
                model=self.config["name"],
                messages=messages,
                temperature=temperature,
            )
            
            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content
            
            # è®°å½•ä½¿ç”¨æƒ…å†µ
            if response.usage:
                self.logger.debug(f"Tokenä½¿ç”¨: è¾“å…¥={response.usage.prompt_tokens}, "
                                f"è¾“å‡º={response.usage.completion_tokens}, "
                                f"æ€»è®¡={response.usage.total_tokens}")
            
            return content
            
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def call_with_system_prompt(self, system_prompt: str, user_prompt: str, 
                               temperature: float = 0.3, 
                               max_tokens: Optional[int] = None) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯è°ƒç”¨LLM
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            
        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return self.call_llm(messages, temperature, max_tokens)
    
    @classmethod
    def from_se_config(cls, se_config: Dict[str, Any], use_operator_model: bool = False) -> "LLMClient":
        """
        ä»SEæ¡†æ¶é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯
        
        Args:
            se_config: SEæ¡†æ¶é…ç½®å­—å…¸
            use_operator_model: æ˜¯å¦ä½¿ç”¨operator_modelsé…ç½®è€Œä¸æ˜¯ä¸»æ¨¡å‹é…ç½®
            
        Returns:
            LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        if use_operator_model and "operator_models" in se_config:
            model_config = se_config["operator_models"]
        else:
            model_config = se_config["model"]
        
        return cls(model_config)


class TrajectorySummarizer:
    """ä¸“é—¨ç”¨äºè½¨è¿¹æ€»ç»“çš„LLMå®¢æˆ·ç«¯åŒ…è£…å™¨"""
    
    def __init__(self, llm_client: LLMClient):
        """
        åˆå§‹åŒ–è½¨è¿¹æ€»ç»“å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        self.llm_client = llm_client
        self.logger = get_se_logger("traj_summarizer", emoji="ğŸ“Š")
    
    def summarize_trajectory(self, trajectory_content: str, patch_content: str, 
                           iteration: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ€»ç»“è½¨è¿¹å†…å®¹
        
        Args:
            trajectory_content: .traæ–‡ä»¶å†…å®¹
            patch_content: .patch/.predæ–‡ä»¶å†…å®¹ (é¢„æµ‹ç»“æœ)
            iteration: è¿­ä»£æ¬¡æ•°
            
        Returns:
            è½¨è¿¹æ€»ç»“å­—å…¸
        """
        from .traj_summarizer import TrajSummarizer
        
        summarizer = TrajSummarizer()
        
        try:
            # è·å–æç¤ºè¯
            system_prompt = summarizer.get_system_prompt()
            user_prompt = summarizer.format_user_prompt(trajectory_content, patch_content)
            
            self.logger.info(f"å¼€å§‹LLMè½¨è¿¹æ€»ç»“ (è¿­ä»£{iteration})")
            
            # è°ƒç”¨LLM
            response = self.llm_client.call_with_system_prompt(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.3,
                max_tokens=2000
            )
            
            # è§£æå“åº”
            summary = summarizer.parse_response(response)
            
            # éªŒè¯å“åº”æ ¼å¼
            if summarizer.validate_response_format(summary):
                self.logger.info(f"LLMè½¨è¿¹æ€»ç»“æˆåŠŸ (è¿­ä»£{iteration})")
                return summary
            else:
                self.logger.warning(f"LLMå“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œä½¿ç”¨å¤‡ç”¨æ€»ç»“ (è¿­ä»£{iteration})")
                return summarizer.create_fallback_summary(trajectory_content, patch_content, iteration)
                
        except Exception as e:
            self.logger.error(f"LLMè½¨è¿¹æ€»ç»“å¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨æ€»ç»“
            return summarizer.create_fallback_summary(trajectory_content, patch_content, iteration)