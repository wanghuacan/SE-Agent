#!/usr/bin/env python3

"""
SE Framework Trajectory Processor

ä¸ºSEæ¡†æ¶æä¾›è½¨è¿¹æ–‡ä»¶å¤„ç†åŠŸèƒ½ï¼Œåœ¨æ¯ä¸ªiterationåç”Ÿæˆç®€åŒ–çš„.traæ–‡ä»¶ã€‚
åŸºäºconverter_old.pyçš„é€»è¾‘ï¼Œé€‚é…SEæ¡†æ¶çš„ç›®å½•ç»“æ„ã€‚
"""

import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
# from .se_logger import get_se_logger


class TrajectoryProcessor:
    """è½¨è¿¹æ–‡ä»¶å¤„ç†å™¨ï¼Œç”¨äºç”Ÿæˆç®€åŒ–çš„.traæ–‡ä»¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–è½¨è¿¹å¤„ç†å™¨"""
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
        try:
            from .se_logger import get_se_logger
            self.logger = get_se_logger("trajectory_processor", emoji="ğŸ¬")
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ—¥å¿—
            import logging
            self.logger = logging.getLogger("trajectory_processor")
            self.logger.setLevel(logging.INFO)
    
    def _count_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenè®¡æ•°è¿‘ä¼¼ç®—æ³•"""
        if not text or not isinstance(text, str):
            return 0
        # åŸºç¡€tokenè®¡æ•° - æŒ‰ç©ºç™½ç¬¦å’Œå¸¸è§æ ‡ç‚¹åˆ†å‰²
        tokens = re.findall(r'\b\w+\b', text.lower())
        return len(tokens)
    
    def _truncate_text(self, text: str, first_percent: float = 0.2, last_percent: float = 0.1) -> str:
        """
        ä½¿ç”¨å­—ç¬¦ç™¾åˆ†æ¯”çº¦æŸæˆªæ–­æ–‡æœ¬å†…å®¹
        
        Args:
            text: è¦æˆªæ–­çš„æ–‡æœ¬
            first_percent: ä¿ç•™å¼€å¤´çš„ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤20%ï¼‰
            last_percent: ä¿ç•™ç»“å°¾çš„ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤10%ï¼‰
            
        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return text
            
        text_length = len(text)
        
        # åªå¯¹è¶³å¤Ÿé•¿çš„å†…å®¹è¿›è¡Œæˆªæ–­
        if text_length < 300:
            return text
        
        # è®¡ç®—é¦–éƒ¨é•¿åº¦ï¼ˆ20%ï¼Œçº¦æŸåœ¨30-150å­—ç¬¦ï¼‰
        first_length = int(text_length * first_percent)
        first_length = max(30, min(150, first_length))
        
        # è®¡ç®—å°¾éƒ¨é•¿åº¦ï¼ˆ10%ï¼Œçº¦æŸåœ¨30-100å­—ç¬¦ï¼‰
        last_length = int(text_length * last_percent)
        last_length = max(30, min(100, last_length))
        
        # æ£€æŸ¥æˆªæ–­æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆä¿ç•™è¶…è¿‡80%æ—¶ä¸æˆªæ–­ï¼‰
        truncated_length = first_length + last_length + len("... [TRUNCATED] ...")
        if truncated_length >= text_length * 0.8:
            return text
            
        # æå–é¦–å°¾éƒ¨åˆ†
        first_part = text[:first_length]
        last_part = text[-last_length:]
        
        # ç»„åˆæˆªæ–­æ ‡è®°
        return f"{first_part}... [TRUNCATED] ...{last_part}"
    
    def _truncate_tool_content(self, content) -> str:
        """æˆªæ–­å·¥å…·è¾“å‡ºå†…å®¹"""
        if not content:
            return content
            
        # å¤„ç†åˆ—è¡¨æ ¼å¼: [{"type": "text", "text": "..."}]
        if isinstance(content, list) and len(content) > 0:
            first_item = content[0]
            if isinstance(first_item, dict) and "text" in first_item:
                text_content = first_item["text"]
                if isinstance(text_content, str):
                    return self._truncate_text(text_content)
        
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼
        if isinstance(content, str):
            return self._truncate_text(content)
        
        return content
    
    def _create_tra_from_traj(self, traj_file: Path, tra_file: Path) -> Dict[str, int]:
        """
        ä».trajæ–‡ä»¶åˆ›å»º.traæ–‡ä»¶ï¼Œåªä¿ç•™historyçš„role/content
        
        Args:
            traj_file: åŸå§‹è½¨è¿¹æ–‡ä»¶è·¯å¾„
            tra_file: ç›®æ ‡.traæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            with open(traj_file, 'r', encoding='utf-8') as f:
                traj_data = json.load(f)
            
            # æå–å¹¶ç®€åŒ–history
            history = traj_data.get('history', [])
            simplified_history = []
            total_tokens = 0
            original_tokens = 0  # åŸå§‹tokenæ•°ç»Ÿè®¡
            
            for item in history:
                if 'role' not in item:
                    continue
                    
                simplified_item = {
                    'role': item['role']
                }
                
                # é¦–å…ˆç»Ÿè®¡åŸå§‹å†…å®¹çš„tokenæ•°
                for field in ['content', 'thought', 'action']:
                    if field in item and item[field]:
                        original_field_str = str(item[field]) if item[field] else ""
                        original_tokens += self._count_tokens(original_field_str)
                
                # æ ¹æ®è§’è‰²ç±»å‹å¤„ç†ä¸åŒå­—æ®µ
                if item['role'] == 'assistant':
                    # assistantè§’è‰²ï¼šæå–thoughtè€Œécontent
                    if 'thought' in item and item['thought']:
                        simplified_item['thought'] = item['thought']
                    
                    # åŒ…å«actionå¹¶åº”ç”¨æˆªæ–­
                    if 'action' in item and item['action']:
                        original_action = item['action']
                        action = original_action
                        
                        # å¯¹str_replace_editoræˆ–é•¿action(>350å­—ç¬¦)åº”ç”¨æˆªæ–­
                        if isinstance(action, str):
                            if 'str_replace_editor' in action or len(action) > 350:
                                action = self._truncate_text(action)
                        elif isinstance(action, dict):
                            action_str = str(action)
                            if 'str_replace_editor' in action_str or len(action_str) > 350:
                                action = self._truncate_text(action_str)
                        
                        simplified_item['action'] = action
                        
                else:
                    # éassistantè§’è‰²ï¼šä½¿ç”¨content
                    if 'content' in item and item['content']:
                        original_content = item['content']
                        content = original_content
                        
                        # å¯¹toolè§’è‰²çš„é•¿è§‚å¯Ÿç»“æœåº”ç”¨æˆªæ–­
                        if item['role'] == 'tool':
                            content = self._truncate_tool_content(content)
                        
                        simplified_item['content'] = content
                
                # åªæ·»åŠ æœ‰æ„ä¹‰å†…å®¹çš„é¡¹ï¼ˆä¸åªæ˜¯roleï¼‰
                if len(simplified_item) > 1:
                    simplified_history.append(simplified_item)
                    
                    # ç»Ÿè®¡å‹ç¼©åå­—æ®µçš„tokenæ•°
                    for field in ['content', 'thought', 'action']:
                        if field in simplified_item:
                            field_str = str(simplified_item[field]) if simplified_item[field] else ""
                            total_tokens += self._count_tokens(field_str)
            
            # åˆ›å»º.traæ–‡ä»¶å†…å®¹
            tra_data = {
                'Trajectory': simplified_history
            }
            
            # å†™å…¥.traæ–‡ä»¶
            with open(tra_file, 'w', encoding='utf-8') as f:
                json.dump(tra_data, f, indent=2)
            
            # è®¡ç®—èŠ‚çœçš„tokenæ•°
            saved_tokens = original_tokens - total_tokens
            compression_ratio = (saved_tokens / original_tokens * 100) if original_tokens > 0 else 0
            
            return {
                'total_tokens': total_tokens,
                'original_tokens': original_tokens,
                'saved_tokens': saved_tokens,
                'compression_ratio': compression_ratio,
                'history_items': len(simplified_history)
            }
            
        except Exception as e:
            self.logger.error(f"åˆ›å»º.traæ–‡ä»¶å¤±è´¥ {traj_file}: {e}")
            return {
                'total_tokens': 0,
                'original_tokens': 0,
                'saved_tokens': 0,
                'compression_ratio': 0,
                'history_items': 0
            }
    
    def process_iteration_directory(self, iteration_dir: Path) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªiterationç›®å½•ï¼Œä¸ºæ‰€æœ‰å®ä¾‹ç”Ÿæˆ.traæ–‡ä»¶
        
        Args:
            iteration_dir: iterationç›®å½•è·¯å¾„ï¼ˆå¦‚iteration_1/ï¼‰
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        self.logger.info(f"å¼€å§‹å¤„ç†iterationç›®å½•: {iteration_dir}")
        
        if not iteration_dir.exists() or not iteration_dir.is_dir():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {iteration_dir}")
            return {}
        
        processing_stats = {
            'iteration_dir': str(iteration_dir),
            'processed_instances': [],
            'total_tokens': 0,
            'total_tra_files': 0,
            'failed_instances': []
        }
        
        # éå†æ‰€æœ‰å®ä¾‹ç›®å½•
        for instance_dir in iteration_dir.iterdir():
            if not instance_dir.is_dir() or instance_dir.name.startswith('.'):
                continue
            
            # æŸ¥æ‰¾.trajæ–‡ä»¶
            traj_files = list(instance_dir.glob("*.traj"))
            if not traj_files:
                self.logger.debug(f"å®ä¾‹ {instance_dir.name} æ²¡æœ‰.trajæ–‡ä»¶")
                continue
            
            instance_stats = {
                'instance_name': instance_dir.name,
                'tra_files_created': [],
                'total_tokens': 0,
                'total_history_items': 0
            }
            
            # å¤„ç†æ¯ä¸ª.trajæ–‡ä»¶
            for traj_file in traj_files:
                tra_file = instance_dir / (traj_file.stem + '.tra')
                
                # ç”Ÿæˆ.traæ–‡ä»¶
                file_stats = self._create_tra_from_traj(traj_file, tra_file)
                
                if file_stats['history_items'] > 0:
                    instance_stats['tra_files_created'].append({
                        'traj_file': traj_file.name,
                        'tra_file': tra_file.name,
                        'tokens': file_stats['total_tokens'],
                        'original_tokens': file_stats['original_tokens'],
                        'saved_tokens': file_stats['saved_tokens'],
                        'compression_ratio': file_stats['compression_ratio'],
                        'history_items': file_stats['history_items']
                    })
                    instance_stats['total_tokens'] += file_stats['total_tokens']
                    instance_stats['total_history_items'] += file_stats['history_items']
                    
                    # æ›´è¯¦ç»†çš„æ—¥å¿—è®°å½•ï¼ŒåŒ…å«èŠ‚çœä¿¡æ¯
                    self.logger.info(f"å·²åˆ›å»º {tra_file.name}: {file_stats['history_items']} å†å²é¡¹, "
                                   f"{file_stats['total_tokens']} tokens "
                                   f"(åŸå§‹: {file_stats['original_tokens']}, "
                                   f"èŠ‚çœ: {file_stats['saved_tokens']}, "
                                   f"å‹ç¼©ç‡: {file_stats['compression_ratio']:.1f}%)")
                else:
                    processing_stats['failed_instances'].append({
                        'instance_name': instance_dir.name,
                        'traj_file': traj_file.name,
                        'reason': 'No valid history items'
                    })
            
            if instance_stats['tra_files_created']:
                processing_stats['processed_instances'].append(instance_stats)
                processing_stats['total_tokens'] += instance_stats['total_tokens']
                processing_stats['total_tra_files'] += len(instance_stats['tra_files_created'])
                
                self.logger.info(f"å®ä¾‹ {instance_dir.name}: åˆ›å»ºäº† "
                               f"{len(instance_stats['tra_files_created'])} ä¸ª.traæ–‡ä»¶")
        
        # è®°å½•å¤„ç†ç»“æœ
        self.logger.info(f"iterationå¤„ç†å®Œæˆ: åˆ›å»ºäº† {processing_stats['total_tra_files']} ä¸ª.traæ–‡ä»¶, "
                        f"æ€»è®¡ ~{processing_stats['total_tokens']} tokens")
        
        if processing_stats['failed_instances']:
            self.logger.warning(f"å¤±è´¥çš„å®ä¾‹æ•°: {len(processing_stats['failed_instances'])}")
        
        return processing_stats
    
    def process_workspace_directory(self, workspace_dir: Path, target_iterations: Optional[List[int]] = None) -> Dict[str, Any]:
        """
        å¤„ç†æ•´ä¸ªworkspaceç›®å½•çš„æ‰€æœ‰iterations
        
        Args:
            workspace_dir: workspaceç›®å½•è·¯å¾„
            target_iterations: æŒ‡å®šè¦å¤„ç†çš„iterationåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
            
        Returns:
            æ•´ä½“å¤„ç†ç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹å¤„ç†workspaceç›®å½•: {workspace_dir}")
        
        if not workspace_dir.exists() or not workspace_dir.is_dir():
            self.logger.error(f"Workspaceç›®å½•ä¸å­˜åœ¨: {workspace_dir}")
            return {}
        
        workspace_stats = {
            'workspace_dir': str(workspace_dir),
            'iterations_processed': [],
            'total_tokens': 0,
            'total_tra_files': 0,
            'processing_errors': []
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰iterationç›®å½•
        iteration_pattern = re.compile(r'^iteration_(\d+)$')
        iteration_dirs = []
        
        for item in workspace_dir.iterdir():
            if item.is_dir():
                match = iteration_pattern.match(item.name)
                if match:
                    iteration_num = int(match.group(1))
                    if target_iterations is None or iteration_num in target_iterations:
                        iteration_dirs.append((iteration_num, item))
        
        # æŒ‰iterationå·æ’åº
        iteration_dirs.sort(key=lambda x: x[0])
        
        if not iteration_dirs:
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•iterationç›®å½•")
            return workspace_stats
        
        # å¤„ç†æ¯ä¸ªiteration
        for iteration_num, iteration_dir in iteration_dirs:
            try:
                iteration_stats = self.process_iteration_directory(iteration_dir)
                if iteration_stats:
                    workspace_stats['iterations_processed'].append({
                        'iteration_number': iteration_num,
                        'stats': iteration_stats
                    })
                    workspace_stats['total_tokens'] += iteration_stats['total_tokens']
                    workspace_stats['total_tra_files'] += iteration_stats['total_tra_files']
            except Exception as e:
                error_info = {
                    'iteration_number': iteration_num,
                    'iteration_dir': str(iteration_dir),
                    'error': str(e)
                }
                workspace_stats['processing_errors'].append(error_info)
                self.logger.error(f"å¤„ç†iteration_{iteration_num}æ—¶å‡ºé”™: {e}")
        
        # æœ€ç»ˆç»Ÿè®¡
        processed_iterations = len(workspace_stats['iterations_processed'])
        self.logger.info(f"Workspaceå¤„ç†å®Œæˆ: {processed_iterations} ä¸ªiteration, "
                        f"{workspace_stats['total_tra_files']} ä¸ª.traæ–‡ä»¶, "
                        f"~{workspace_stats['total_tokens']} tokens")
        
        return workspace_stats
    
    def extract_problem_from_tra(self, tra_file: Path, problem_file: Path) -> bool:
        """
        ä».traæ–‡ä»¶ä¸­æå–problemæè¿°å¹¶ä¿å­˜ä¸º.problemæ–‡ä»¶
        
        Args:
            tra_file: .traæ–‡ä»¶è·¯å¾„
            problem_file: ç›®æ ‡.problemæ–‡ä»¶è·¯å¾„
            
        Returns:
            Trueè¡¨ç¤ºæˆåŠŸæå–ï¼ŒFalseè¡¨ç¤ºå¤±è´¥
        """
        try:
            with open(tra_file, 'r', encoding='utf-8') as f:
                tra_data = json.load(f)
            
            # å®šä½åˆ°Trajectory[1]["content"][0]["text"]
            trajectory = tra_data.get('Trajectory', [])
            if len(trajectory) < 2:
                self.logger.warning(f"traæ–‡ä»¶æ ¼å¼å¼‚å¸¸ï¼Œtrajectoryé•¿åº¦ä¸è¶³: {tra_file}")
                return False
            
            user_entry = trajectory[1]
            if user_entry.get('role') != 'user':
                self.logger.warning(f"trajectory[1]ä¸æ˜¯userè§’è‰²: {tra_file}")
                return False
            
            content = user_entry.get('content', [])
            if not isinstance(content, list) or len(content) == 0:
                self.logger.warning(f"user contentæ ¼å¼å¼‚å¸¸: {tra_file}")
                return False
            
            text_content = content[0].get('text', '')
            if not text_content:
                self.logger.warning(f"æœªæ‰¾åˆ°textå†…å®¹: {tra_file}")
                return False
            
            # ä½¿ç”¨æ­£åˆ™æå–<pr_description>æ ‡ç­¾ä¸­çš„å†…å®¹
            import re
            match = re.search(r'<pr_description>\s*(.*?)\s*</pr_description>', text_content, re.DOTALL)
            if not match:
                self.logger.warning(f"æœªæ‰¾åˆ°pr_descriptionæ ‡ç­¾: {tra_file}")
                return False
            
            problem_description = match.group(1).strip()
            if not problem_description:
                self.logger.warning(f"pr_descriptionå†…å®¹ä¸ºç©º: {tra_file}")
                return False
            
            # å†™å…¥.problemæ–‡ä»¶
            with open(problem_file, 'w', encoding='utf-8') as f:
                f.write(problem_description)
            
            # ç»Ÿè®¡ä¿¡æ¯
            problem_tokens = self._count_tokens(problem_description)
            self.logger.info(f"å·²æå–problem: {problem_file.name} ({problem_tokens} tokens)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æå–problemå¤±è´¥ {tra_file}: {e}")
            return False
    
    def process_problems_in_iteration(self, iteration_dir: Path) -> Dict[str, Any]:
        """
        ä¸ºiterationç›®å½•ä¸­çš„æ‰€æœ‰å®ä¾‹æå–problemæ–‡ä»¶
        
        Args:
            iteration_dir: iterationç›®å½•è·¯å¾„
            
        Returns:
            æå–ç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æå–iterationç›®å½•çš„problems: {iteration_dir}")
        
        if not iteration_dir.exists() or not iteration_dir.is_dir():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {iteration_dir}")
            return {}
        
        problem_stats = {
            'iteration_dir': str(iteration_dir),
            'problems_extracted': [],
            'total_problems': 0,
            'failed_extractions': []
        }
        
        # éå†æ‰€æœ‰å®ä¾‹ç›®å½•
        for instance_dir in iteration_dir.iterdir():
            if not instance_dir.is_dir() or instance_dir.name.startswith('.'):
                continue
            
            # æŸ¥æ‰¾.traæ–‡ä»¶
            tra_files = list(instance_dir.glob("*.tra"))
            if not tra_files:
                self.logger.debug(f"å®ä¾‹ {instance_dir.name} æ²¡æœ‰.traæ–‡ä»¶")
                continue
            
            # å¤„ç†æ¯ä¸ª.traæ–‡ä»¶ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
            for tra_file in tra_files:
                problem_file = instance_dir / (instance_dir.name + '.problem')
                
                success = self.extract_problem_from_tra(tra_file, problem_file)
                
                if success:
                    problem_stats['problems_extracted'].append({
                        'instance_name': instance_dir.name,
                        'tra_file': tra_file.name,
                        'problem_file': problem_file.name
                    })
                    problem_stats['total_problems'] += 1
                else:
                    problem_stats['failed_extractions'].append({
                        'instance_name': instance_dir.name,
                        'tra_file': tra_file.name,
                        'reason': 'Problem extraction failed'
                    })
        
        self.logger.info(f"problemæå–å®Œæˆ: æˆåŠŸ {problem_stats['total_problems']} ä¸ª, "
                        f"å¤±è´¥ {len(problem_stats['failed_extractions'])} ä¸ª")
        
        return problem_stats


def process_trajectory_files(workspace_dir: str, iterations: Optional[List[int]] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¤„ç†è½¨è¿¹æ–‡ä»¶
    
    Args:
        workspace_dir: workspaceç›®å½•è·¯å¾„
        iterations: è¦å¤„ç†çš„iterationåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    processor = TrajectoryProcessor()
    return processor.process_workspace_directory(Path(workspace_dir), iterations)


def extract_problems_from_workspace(workspace_dir: str, iterations: Optional[List[int]] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»workspaceæå–problemæ–‡ä»¶
    
    Args:
        workspace_dir: workspaceç›®å½•è·¯å¾„
        iterations: è¦å¤„ç†çš„iterationåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
        
    Returns:
        æå–ç»“æœç»Ÿè®¡
    """
    import re
    
    processor = TrajectoryProcessor()
    workspace_path = Path(workspace_dir)
    
    if not workspace_path.exists():
        return {'error': f'Workspaceç›®å½•ä¸å­˜åœ¨: {workspace_path}'}
    
    # æŸ¥æ‰¾iterationç›®å½•
    iteration_pattern = re.compile(r'^iteration_(\d+)$')
    iteration_dirs = []
    
    for item in workspace_path.iterdir():
        if item.is_dir():
            match = iteration_pattern.match(item.name)
            if match:
                iteration_num = int(match.group(1))
                if iterations is None or iteration_num in iterations:
                    iteration_dirs.append((iteration_num, item))
    
    iteration_dirs.sort(key=lambda x: x[0])
    
    workspace_results = {
        'workspace_dir': str(workspace_path),
        'iterations_processed': [],
        'total_problems': 0,
        'total_failed': 0
    }
    
    for iteration_num, iteration_dir in iteration_dirs:
        problem_stats = processor.process_problems_in_iteration(iteration_dir)
        if problem_stats:
            workspace_results['iterations_processed'].append({
                'iteration_number': iteration_num,
                'stats': problem_stats
            })
            workspace_results['total_problems'] += problem_stats.get('total_problems', 0)
            workspace_results['total_failed'] += len(problem_stats.get('failed_extractions', []))
    
    return workspace_results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¤„ç†Demo_Structureç›®å½•
    demo_workspace = "/home/uaih3k9x/630_swe/SE/trajectories/Demo_Structure"
    
    processor = TrajectoryProcessor()
    results = processor.process_workspace_directory(Path(demo_workspace))
    
    print("å¤„ç†ç»“æœ:")
    print(f"- å¤„ç†çš„iterations: {len(results['iterations_processed'])}")
    print(f"- åˆ›å»ºçš„.traæ–‡ä»¶: {results['total_tra_files']}")
    print(f"- æ€»tokenæ•°: ~{results['total_tokens']}")